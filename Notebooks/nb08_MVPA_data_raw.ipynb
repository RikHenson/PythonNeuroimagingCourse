{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- **Created:** [Rik Henson](https://www.mrc-cbu.cam.ac.uk/people/rik.henson/) with thanks to [Danny Mitchell](https://www.mrc-cbu.cam.ac.uk/people/Daniel.Mitchell)\n",
    "- **Date:** November 2024\n",
    "- **conda environment**: This uses the [mri environment](https://github.com/RikHenson/PythonNeuroimagingCourse/blob/main/mri_environment.yml)\n",
    "\n",
    "# Multi-voxel Classification Analyses (MVPA): real data\n",
    "\n",
    "This notebook applies MVPA to real data, following the previous notebook explaining MVPA in general. Much of this tutorial is inspired by the longer fMRI-pattern-analysis course developed by Lukas Snoek at the University of Amsterdam: https://lukas-snoek.com/NI-edu/index.html. \n",
    "\n",
    "- **Last updated:** November 2024 by [Rik Henson](https://www.mrc-cbu.cam.ac.uk/people/rik.henson/) \n",
    "- **conda environment**: Needs the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 0. Getting Ready\n",
    "\n",
    "As usual, we need some python packages like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # plotting\n",
    "# to show plots in cell\n",
    "%matplotlib inline   \n",
    "\n",
    "import os           # To interact with the operating system, including files and paths (e.g. path.join)\n",
    "import bids.layout  # To fetch data from BIDS-compliant datasets\n",
    "import numpy        # This lets python process arrays/matrices\n",
    "import pandas       # To use \"dataframes\"      \n",
    "import seaborn      # This provides another popular set of plotting functions\n",
    "import time                     # To use \"time\" (like \"tic\" in Matlab)\n",
    "\n",
    "import nibabel      # Basic nifti image utilities\n",
    "from nibabel.affines import apply_affine # for transforming coordinates\n",
    "\n",
    "import nilearn      # Many useful functions for MRI, including...\n",
    "from nilearn import image       # to load (load_img), resample (resample_to_img), manipulate (math_img) fMRI data, etc.\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn import plotting    # includes plot_roi, plot_stat_map, view_img_on_surf, etc.\n",
    "from nilearn import input_data  # loading files\n",
    "from nilearn import decoding    # includes Searchlight\n",
    "\n",
    "import sklearn \n",
    "from sklearn import preprocessing   # includes LabelEncoder, OneHotEncoder, Standardzscorer...\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set-up our input and output directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wd = '/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceRecognition/' # <-- CHANGE TO YOURS\n",
    "out_dir = os.path.join(wd, 'results', 'mvpa')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "os.chdir(out_dir)\n",
    "print(f\"Working directory currently {os.getcwd()}\")\n",
    "\n",
    "fmri_data_dir = os.path.join(wd, 'data') # data in BIDS format\n",
    "fmri_results_dir = os.path.join(wd, 'results') # results in BIDS format\n",
    "\n",
    "layout = bids.layout.BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "layout.add_derivatives(os.path.join(fmri_results_dir, \"first-level\"))\n",
    "\n",
    "TR = layout.get_tr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting trial-specific responses (Betas)\n",
    "\n",
    "We are going to try to classify famous versus scrambled faces. First we first need to extract estimates for every trial that contained one of these, which involves re-creating and re-fitting a different design matrix. We will use subject 15 as in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sID = '15'\n",
    "bold = layout.get(subject = sID, datatype = 'func', desc = 'preproc', extension = '.nii.gz', return_type = 'filename')\n",
    "print(\"Found \" + str(len(bold)) + \" preprocessed functional files\")\n",
    "\n",
    "events_files = layout.get(subject=sID, datatype='func', suffix='events', extension=\".tsv\", return_type='filename')\n",
    "print(\"Found \" + str(len(events_files)) + \" event files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to focus on just trials involving Famous Faces (FF) and Scrambled Faces (SF), and more specifically their Initial presentation (Ini) and Delayed repetition (Del). The reason we are not going to estimate Unfamiliar faces is that we are going to classify multi-voxel patterns for faces versus scrambled faces, and want roughly equal numbers of each class (i.e, do not want twice as many faces as scrambled faces; see later). The reason we are not going to try to estimate BOLD responses to Immediate repeats is that they always follow initial presentations close in time, which means that their single-trial regressors (see below) are highly correlated, such that the response estimate for an initial trial is difficult to separate from that for its immediate repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['IniFF', 'ImmFF', 'DelFF', 'IniUF', 'ImmUF', 'DelUF', 'IniSF', 'ImmSF', 'DelSF']\n",
    "conditions_of_interest = ['IniFF', 'DelFF', 'IniSF', 'DelSF']\n",
    "other_conditions = [val for val in conditions if val not in conditions_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Least Squares All (LSA) GLM\n",
    "\n",
    "One way to estimate the response to individual trials is **Least-Squares-All** (LSA), which simply involves creating a new regressor for each trial (i.e, containing a single event onset). This can be appropriate in some situations, but generally results in quite noisy Beta estimates (particularly when the time between events is short - see for example [Abdulrahman & Henson, 2016](https://pubmed.ncbi.nlm.nih.gov/26549299/)). An alternative is **Least-Squares-Separate** (LSS), which can improve estimates (by temporal regularisation), but is much more computationally expensive. In the interests of time, we will use LSA here, but see [https://nilearn.github.io/dev/auto_examples/07_advanced/plot_beta_series.html](https://nilearn.github.io/dev/auto_examples/07_advanced/plot_beta_series.html) for an LSS function.\n",
    "\n",
    "First we need to give every famous and scrambled trial a unique name (based on the stimulus that was presented), as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_events = []\n",
    "for run, events_file in enumerate(events_files):\n",
    "    events_df = pandas.read_table(events_file)\n",
    "    for j, event in enumerate(events_df['trial_type']):\n",
    "        if event in conditions_of_interest:\n",
    "            events_df.loc[j, 'trial_type'] = event + events_df['stim_file'][j][-8:-4]\n",
    "    events_df = events_df.drop(columns = ['button_pushed', 'stim_file', 'trigger', 'circle_duration', 'response_time'])\n",
    "    lsa_events.append(events_df)\n",
    "\n",
    "print(lsa_events[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the trials of no interest are still modelled in a single regressor per trial-type. We also need to re-get the confounds, like in the previous single-subject analysis notebook. Here is the resulting design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create design matrix for first run\n",
    "nvols = nibabel.load(bold[0]).shape[-1]\n",
    "frame_times = numpy.linspace(0, (nvols - 1) * TR, nvols)\n",
    "design_matrix = nilearn.glm.first_level.make_first_level_design_matrix(frame_times, events = events_df, hrf_model = 'glover', drift_model = 'cosine', high_pass = 0, drift_order = None)\n",
    "nilearn.plotting.plot_design_matrix(design_matrix, output_file=None)\n",
    "fig = plt.gcf(); fig.set_size_inches(8,2); plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit such a GLM to the data, as in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the confounds of interest\n",
    "confound_files = layout.get(subject = sID, datatype = 'func', desc = 'confounds', extension = \".tsv\", return_type = 'filename')\n",
    "relevant_confounds = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "confounds_for_glm = []\n",
    "for conf_file in confound_files:\n",
    "    this_conf = pandas.read_table(conf_file)\n",
    "    conf_subset = this_conf[relevant_confounds].fillna(0) # replace NaN with 0\n",
    "    confounds_for_glm.append(conf_subset)\n",
    "\n",
    "# reference slice\n",
    "slice_timing = layout.get_metadata(bold[0])\n",
    "if slice_timing['SliceTimingCorrected']:\n",
    "  slice_time_ref = slice_timing['StartTime'] / TR\n",
    "  print('Slice timing reference:', slice_time_ref)\n",
    "\n",
    "# Get brain mask\n",
    "brain_mask_file = layout.get(return_type='file', datatype='func', suffix='mask', desc='brain', space='MNI152NLin2009cAsym', extension='nii.gz')[0]\n",
    "brain_mask = nibabel.load(brain_mask_file)\n",
    "\n",
    "# Set-up GLM parameters\n",
    "fmri_glm = FirstLevelModel(t_r = TR,\n",
    "                           slice_time_ref = slice_time_ref,\n",
    "                           hrf_model = 'spm',\n",
    "                           drift_model = 'cosine',\n",
    "                           drift_order = 1,\n",
    "                           high_pass = 0.01,\n",
    "                           smoothing_fwhm = None,\n",
    "                           signal_scaling = (0, 1), # grand mean scaling only\n",
    "                           noise_model = 'ols', # no need for 'ar1' if only care about Betas, should speed up\n",
    "                           mask_img = brain_mask,\n",
    "                           memory = 'scratch');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are 1) not smoothing (since we do not necessarily want to smooth pattern information, though see [Kriegeskorte et al (2009)](https://pmc.ncbi.nlm.nih.gov/articles/PMC2818340/) for discussion about smoothness of fMRI patterns), 2) using the quicker \"ols\" rather than \"ar1\" model of the error autocorrelation (since we do not care about precise statistics; only the Betas) and 3) only scaling to the mean over voxels and TRs in each run (the \"(0,1)\" option for the \"signal_scaling\" variable), since we do not want to adjust for global changes that might be caused by trial-specific responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready output directory\n",
    "model_name = 'LSA_2FF_2SF' \n",
    "beta_dir = os.path.join(out_dir, model_name, 'sub-' + sID)\n",
    "if not os.path.exists(beta_dir):\n",
    "    os.makedirs(beta_dir)\n",
    "\n",
    "affine = nibabel.load(bold[0]).affine # need for writing 4D images below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run through all 9 runs, fitting the GLM, calculating the effect size (beta) for each initial FF and UF trial (to save in a 4D image for each run). Note this will take a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_per_run_files = []\n",
    "for run in range(len(bold)):\n",
    "    # fit GLM\n",
    "    run_glm = fmri_glm.fit(bold[run], lsa_events[run], confounds_for_glm[run]);\n",
    "    \n",
    "    # Compute betas for each initial FF and UF trial\n",
    "    trials_to_save = lsa_events[run][\"trial_type\"]\n",
    "    trials_to_save = [val for val in trials_to_save if val not in other_conditions]\n",
    "    lsa_beta_maps  = [] \n",
    "    print(f'Run {run}: Creating {len(trials_to_save)} beta images...')\n",
    "    for trial in trials_to_save:\n",
    "        beta_map = run_glm.compute_contrast(trial, output_type = 'effect_size');\n",
    "        lsa_beta_maps.append(beta_map)\n",
    "\n",
    "    # save trials as a 4D image so can load later\n",
    "    print(f'Run {run+1}: Saving to a 4D file...')\n",
    "    volumes = numpy.squeeze([lsa_beta_maps[trial].get_fdata() for trial in range(0,len(lsa_beta_maps))])\n",
    "    data_4d = numpy.stack(volumes, axis=-1)  # Stack along the 4th dimension\n",
    "    img_4d  = nibabel.Nifti1Image(data_4d, affine);\n",
    "    betas_per_run_files.append(os.path.join(beta_dir, os.path.basename(bold[run]).split(\"space\")[0] + model_name + '_betas.nii.gz'))\n",
    "    nibabel.save(img_4d, betas_per_run_files[run]);\n",
    " \n",
    "# Remove the cached directory\n",
    "!rm -rf scratch/joblib/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you have already created the Beta images, you can simply load them below by running these lines:\n",
    "\n",
    "```python\n",
    "brain_masker = nilearn.input_data.NiftiMasker\n",
    "betas_per_run_files = []\n",
    "for run in range(len(bold)):\n",
    "    betas_per_run_files.append(os.path.join(beta_dir, os.path.basename(bold[run]).split(\"space\")[0] + model_name + '_betas.nii.gz'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defining the class labels\n",
    "\n",
    "We also need to define the labels associated with each trial, which we will need for classification later, using sklearn's function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_enc = sklearn.preprocessing.LabelEncoder() # this initialises the LabelEncoder object\n",
    "labels_per_run = []\n",
    "for run in range(len(bold)):\n",
    "    trials_to_save = lsa_events[run][\"trial_type\"]\n",
    "    trials_to_save = [val for val in trials_to_save if val not in other_conditions]\n",
    "    trials_to_save = [label[3:5] for label in trials_to_save]\n",
    "    labels_per_run.append(lab_enc.fit_transform(trials_to_save)) # Encode as integers, done alphabetically, so FF=0, SF=1\n",
    "\n",
    "print(\"First 10 condition labels for last run: \" + str(trials_to_save[0:10]))\n",
    "print(\"First 10 class labels for first run: \" + str(labels_per_run[-1][0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Defining the ROI\n",
    "\n",
    "Finally we load up the right fusiform ROI we created in the ROI notebook. Note that since this ROI was defined by the contrast of Faces > Scrambled, it would not be surprising if we could classify these two classes within this ROI (ie the selection is biased). However, we are going to remove the mean from each condition later, so we are classifying on the patterns only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fusi_ROI = nibabel.load(os.path.join(fmri_results_dir,'sphere_and_faces-scrambled_Zmap_fdr.nii.gz'))\n",
    "fusi_ROI = nibabel.load(os.path.join(fmri_results_dir,'sphere_mask.nii.gz'))\n",
    "# shouldn't need to resample fusi ROI, but just to be safe!\n",
    "fusi_ROI = nilearn.image.resample_to_img(fusi_ROI, betas_per_run_files[0], interpolation='nearest') \n",
    "fusi_masker = nilearn.input_data.NiftiMasker(fusi_ROI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this data from this ROI for the first run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_images = nilearn.image.load_img(betas_per_run_files[0])\n",
    "beta_patterns = fusi_masker.fit_transform(beta_images) \n",
    "\n",
    "ax = seaborn.heatmap(beta_patterns, cmap='coolwarm', center=0, robust=1, xticklabels=50, yticklabels=10)\n",
    "ax.set(xlabel=\"voxels\", ylabel=\"samples / patterns / faces\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of these voxels are correlated, and some will be noisier than others. Classifiers are often able to handle this, but we may choose to apply **dimension reduction**. This is sometimes called **feature extraction**, and a common method is principle component analysis (**PCA**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscorer = sklearn.preprocessing.StandardScaler()\n",
    "beta_patterns = zscorer.fit_transform(beta_patterns)\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=10)\n",
    "pca.fit(beta_patterns)\n",
    "beta_patterns_PCA10 = pca.transform(beta_patterns)\n",
    "\n",
    "ax = seaborn.heatmap(beta_patterns_PCA10, cmap='coolwarm', center=0, robust=1, xticklabels=1, yticklabels=10)\n",
    "ax.set(xlabel=\"Principal components\", ylabel=\"labels\");\n",
    "ax.set_yticks(range(len(labels_per_run[0])));\n",
    "ax.set_yticklabels(labels_per_run[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that features (voxels) are typically standardized before PCA, otherwise the principal components will be dominated by features with larger scales. However, standardization means that voxels that originally had low variance will be upweighted, under the assumption that all features are expected to be equally predictive. In fMRI, noisy/non-predictive voxels may have low variance, so upweighting them can also be detrimental to performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preparing patterns for all runs\n",
    "\n",
    "Now let's put everything together for all runs. However, we are going to add an additional step of removing the mean across voxels for each trial (using the \"demeaner\" transform below, applied across columns). The latter will remove any difference in the mean across FF and SF trials, so that we cannot classify them on the basis of the difference in means that defined the ROI. (Note that when we also apply the zscorer transform, we are removing the mean across trials for each voxel, ie applied across rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demeaner = sklearn.preprocessing.StandardScaler(with_mean=True, with_std=False)\n",
    "nruns = len(betas_per_run_files)\n",
    "\n",
    "beta_patterns_per_run = [] # list of ROI's pattern matrices (one for each run)\n",
    "for run in range(nruns):\n",
    "    print(betas_per_run_files[run])\n",
    "    beta_image = nilearn.image.load_img(betas_per_run_files[run]) \n",
    "    beta_patterns = fusi_masker.fit_transform(beta_image)\n",
    "    beta_patterns = demeaner.fit_transform(beta_patterns.transpose()).transpose() # remove mean across voxels\n",
    "    beta_patterns_per_run.append(beta_patterns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Classifying using leave-one-run-out cross-validation\n",
    "\n",
    "Now we can see how well we can classify famous faces and scrambled faces based on their patterns only (e.g, based on the covariance between voxels). We will concatenate all trials and labels across runs, but also generate an array that indicates which run each trial comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data    = numpy.concatenate(beta_patterns_per_run, axis=0) # concatenate data across runs\n",
    "all_labels  = numpy.concatenate(labels_per_run, axis=0) # concatenate labels across runs\n",
    "run_indices = numpy.concatenate([[i] * numpy.size(beta_patterns_per_run[i],axis=0) for i in range(nruns)]) # label each run number\n",
    "#print(run_indices)\n",
    "\n",
    "print(f\"Total of {all_data.shape[0]} trials\")\n",
    "# If you want to check means of two conditions equal at 0:\n",
    "#fac = all_data[all_labels==0]\n",
    "#scr = all_data[all_labels==1]\n",
    "#print(numpy.mean(fac), numpy.mean(scr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to build a classification pipeline (like in the previous notebook on simulated data), using Z-scoring, PCA and Logistic Regression. Then we are going to use leave-one-run-out cross-validation (normally called \"leave-one-group-out\" or LOGO) and measure area-under-the-ROC (AUROC), averaged over the 9 held-out runs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(max_iter=10000, tol=0.1)\n",
    "pipe = sklearn.pipeline.Pipeline(steps=[(\"zscorer\", zscorer), (\"pca\", pca), (\"Logistic\", logistic)]) # zscorer and pca defined earlier\n",
    "\n",
    "logo = sklearn.model_selection.LeaveOneGroupOut()\n",
    "fold_score = sklearn.model_selection.cross_val_score(pipe, all_data, all_labels, groups = run_indices, scoring = 'roc_auc', cv = logo)\n",
    "observed_score = numpy.mean(fold_score)\n",
    "print(\"Cross-validated score per fold:\", fold_score)  \n",
    "print(\"Mean cross-validated score:  \", observed_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not fantastic, but appears to be above 50% (chance) in most runs. We will test this formally in next section. Of course, classification can also be repeated and averaged across all subjects if more power wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Significance testing\n",
    "\n",
    "The mean AUC above is only just above chance. Is it significantly above chance? We cannot test this using a t-test across folds, because the folds are not independent (they share training data). If we had multiple participants we could use a **one-sample t-test across participants**, versus chance. This is valid (assuming the specified chance level is correct) because participants are independent. \n",
    "\n",
    "For \"within-context\" cross-validation, below-chance accuracy is not meaningful (the true value can't be negative), which means we can use a one-tailed test; it also means that the test only provides fixed-effects rather than random-effects inference (see [Allefeld et al., 2016](https://pubmed.ncbi.nlm.nih.gov/27450073/)). For cross-validation that generalises *across* contexts (e.g. train the classifer to decode the expression of younger faces, then test decoding of the expression of older faces) negative performance *could* be meaningful, so random-effects inference is justified. Similarly, when comparing classfication performance across conditions (e.g. is the performance of expression decoding for younger faces different from that for older faces?), paired-tests or repeated-measures ANOVA allow random-effects inference.\n",
    "\n",
    "If we want **to assess significance for a single participant, we would need a permutation test**. The permutation test involves shuffling the labels many times (e.g. 10,000) to generate a null distribution of classifier performance that would be expected under the null hypothesis of no difference between classes. See the notebook on statistics.\n",
    "\n",
    "First we'll code this explicitly within the cross-validation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = tuple(logo.split(all_data, all_labels, run_indices))\n",
    "\n",
    "n_permutations = 200 # this is not nearly enough, but it will still take a while\n",
    "permuted_scores = numpy.zeros(n_permutations)\n",
    "print(f'Permuting {n_permutations} times:')\n",
    "tic = time.time()\n",
    "for p in range(n_permutations):   \n",
    "    fold_score=numpy.full(nruns,numpy.nan)\n",
    "    for i, fold in enumerate(folds):\n",
    "        train_idx, test_idx = fold   \n",
    "\n",
    "        #### previously we trained the classifier like this:\n",
    "        # pipe.fit( all_data[train_idx,:], all_labels[train_idx] )\n",
    "        #### now we want to do something like this:\n",
    "        # pipe.fit( all_data[train_idx,:], numpy.random.shuffle(all_labels[train_idx]) )\n",
    "        #### but now we need to break it up like this:\n",
    "        train_labels = all_labels[train_idx] \n",
    "        numpy.random.shuffle(train_labels)\n",
    "        pipe.fit(all_data[train_idx,:], train_labels)\n",
    "        #### because indexing makes a temporary copy, and shuffle operates in-place\n",
    "\n",
    "        predicted_probabilities = pipe.predict_proba(all_data[test_idx,:]) # do prediction on test data\n",
    "        fold_score[i] = sklearn.metrics.roc_auc_score(all_labels[test_idx], predicted_probabilities[:,1]) # syntax for binary classifcation\n",
    "    permuted_scores[p] = numpy.mean(fold_score)\n",
    "    print('.',end='')\n",
    "print('\\nTook ', time.time()-tic,' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the observed score and compare it to the null distribution of scores from permuted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seaborn.histplot(permuted_scores, element='step', alpha=0.5)\n",
    "ax.set(xlabel = 'AUROC of permuted data');\n",
    "lh = ax.axvline(numpy.percentile(permuted_scores, 95), color='b',label='95% threshold');\n",
    "mh = ax.plot(observed_score, 0, marker='o', color='r', markersize=10, label='observed score');\n",
    "ax.legend();\n",
    "\n",
    "p = (sum(permuted_scores > observed_score) + 1) / (n_permutations + 1)\n",
    "print('p vlaue = ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the classification was significantly above chance.\n",
    "\n",
    "Just as scikit-learn has a function (`cross_val_score`) to simplify the cross-validation loop, it also has a similar function (`permutation_test_score`) to simplify the permutation process. This also allows the permutation to be run in parallel, which can be much faster! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutations = 1000 # this is more like it, but it will still take ~1min\n",
    "\n",
    "tic = time.time()\n",
    "actual_score, permuted_scores, p = sklearn.model_selection.permutation_test_score(pipe, all_data, all_labels, groups = run_indices,\n",
    "                                   scoring = 'roc_auc', cv = logo, n_permutations = n_permutations, n_jobs = -1, random_state = None)\n",
    "print('Took ', time.time()-tic,' s')\n",
    "print(\"p value = \", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Searchlight analysis\n",
    "\n",
    "So far we have considered a single ROI. If we want to know where in the brain a representation is strongest, we can compare multiple ROIs. Sometimes we might be interested in discrete regions that cover the brain (e.g. an atlas/parcellation), but if we want a spatially continuous output, without making assumptions about area borders, we can use a \"searchlight\". \n",
    "\n",
    "A searchlight is just a set of overlapping ROIs, often spherical, that cover the brain (or analysis mask of interest). Each searchlight's classification performance is typically assigned to its central voxel. The inferences one would make are very similar to a voxel-wise, mass-univariate analysis of smoothed data. Increasing the size of the searchlight reduces the spatial specificity of the inference, while tending to increase sensitivity, in a very similar way to increasing the smoothing kernal in univariate analysis. Similarly to the matched-filter theorem for univariate analyses, the optimal searchlight size will depend on the spatial scale of the signal and the spatial scale of the noise. It is sometimes assumed that searchlights need to be spatially contiguous, because they usually are, but the definition of a searchlight is just as flexible as the definition of any ROI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Defining the search space\n",
    "\n",
    "A searchlight analysis can be set up using nilearn's `.searchlight` object. This *is* restricted to contiguous spherical searchlights. When the searchlight object is created, it needs two masks:\n",
    "\n",
    "1. A \"voxel\" mask containing valid voxels for defining patterns\n",
    "2. A \"process\" mask defining where the sphere centres can be\n",
    "\n",
    "We will use a gray-matter mask for the voxel mask, but just one slice of the gray-matter mask as the process mask. The latter is just to save time (since the search-light can take a long time to run across the whole-brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_gm_mask = nilearn.datasets.load_mni152_gm_mask()\n",
    "voxel_gm_mask = nilearn.image.resample_to_img(voxel_gm_mask, betas_per_run_files[0], interpolation='nearest')\n",
    "\n",
    "process_mask = nilearn.image.get_data(voxel_gm_mask).astype(int)\n",
    "picked_slice = 29\n",
    "process_mask[..., (picked_slice + 1):] = 0\n",
    "process_mask[..., :picked_slice] = 0\n",
    "process_mask = nilearn.image.new_img_like(voxel_gm_mask, process_mask)\n",
    "\n",
    "display = nilearn.plotting.plot_roi(nilearn.image.math_img('a+2*b', a=process_mask, b=voxel_gm_mask), alpha=0.6, cmap='summer', draw_cross=False);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, green voxels could be part of the spherical search-light (depending on its radius), but the centre of the spheres will only ever be placed in the yellow voxels (in the slice corresponding to z=-20).\n",
    "\n",
    "Now we can create the `searchlight` object. We will also specify some other optional inputs, explained in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = nilearn.decoding.SearchLight(\n",
    "    mask_img = voxel_gm_mask,         # (only GM voxels)\n",
    "    process_mask_img = process_mask,  # (only one slice)\n",
    "    radius = 12,                      # in mm\n",
    "    estimator = pipe,                 # a classifier or pipeline object\n",
    "    n_jobs = 1,                       # how many CPUs to use (-1 means the maximum available)\n",
    "    scoring = 'roc_auc',              # choice of scoring metric\n",
    "    cv = logo,                        # cross-validation object\n",
    "    verbose = True                    # this is slow, so if running locally (1 job) set it to true to know how far it's got; when running in parallel it produces too much output, so set to false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Running search\n",
    "\n",
    "We can now concatenate all the beta images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gather data but now as images rather than numpy arrays\n",
    "all_data = nilearn.image.concat_imgs(betas_per_run_files)\n",
    "print('Shape of 4D nifti data:', all_data.shape)\n",
    "print('Shape of mask:', sl.mask_img.shape)\n",
    "print('Shape of process_mask:', sl.process_mask_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and pass them to the `searchlight` object's `.fit` method, passing it the images containing the patterns, labels and groups for cross-validation. This will take a while (depending on how many cores you have, whether you run in parallel, etc)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings # just to stop warnings near edge of image\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Running searchlight...')\n",
    "tic = time.time()\n",
    "#with io.capture_output() as result:\n",
    "sl.fit(all_data, all_labels, groups = run_indices)\n",
    "print('Took ', time.time()-tic,' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output (here, mean AUROC) per voxel is stored in the  `scores_` property of the searchlight object (as a 3D numpy array). If we convert this to a nifti volume, we can plot it using nilearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sl.scores_ # - 0.5 # if want to subtract chance\n",
    "score_img = nilearn.image.new_img_like(voxel_gm_mask, scores) # save in samme format as the brain_mask\n",
    "score_img = nilearn.image.math_img('score_img * process_mask', score_img = score_img, process_mask = sl.process_mask_img)\n",
    "\n",
    "mni_coords = apply_affine(affine, [0, 0, picked_slice])\n",
    "zval = numpy.array([round(mni_coords[2])])\n",
    "\n",
    "# set voxels outside the processing mask to zero\n",
    "plt = nilearn.plotting.plot_stat_map(score_img, threshold=0.5, vmax=1, cmap='hot', cut_coords=zval, display_mode=\"z\") #cut_coords=(-18, -31, -16));cmap='bwr', "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see reasonable classification in right and left fusiform, as expected (the significance of these would require permutation testing for each search-light, which would take even longer...!)\n",
    "\n",
    "This is the end of this MVPA-data notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
