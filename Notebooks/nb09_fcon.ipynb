{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Functional Connectivity\n",
    "\n",
    "- **Created:** November 2024 by [Rik Henson](https://www.mrc-cbu.cam.ac.uk/people/rik.henson/) \n",
    "- **conda environment**: Needs the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-series regression\n",
    "\n",
    "Let's load up the single-trial Betas from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Getting Ready\n",
    "\n",
    "As usual, we need some python packages like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np              # This lets python process matrices, like Matlab\n",
    "import matplotlib.pyplot as plt # This lets python plot graphs like Matlab\n",
    "import seaborn as sns           # This provides another popular set of plotting functions\n",
    "import pandas as pd             # To use \"dataframes\" (like tables in Matlab)\n",
    "import os                       # To interact with the operating system, including files and paths (e.g. path.join)\n",
    "import time                     # To use \"time\" (like \"tic\" in Matlab)\n",
    "\n",
    "# Nilearn modules, for the analysis of brain volumes, plotting, etc., https://nilearn.github.io/\n",
    "import nilearn as nl           # Many useful functions for MRI, including...\n",
    "from nilearn import image       # to load (load_img), resample (resample_to_img), manipulate (math_img) fMRI data, etc.\n",
    "from nilearn import datasets    # includes e.g. fetch_atlas_harvard_oxford\n",
    "from nilearn import masking\n",
    "from nilearn import input_data  # includes NiftiMasker, NiftiLabelsMasker\n",
    "from nilearn import plotting    # includes plot_roi, plot_stat_map, view_img_on_surf, etc.\n",
    "from nilearn import decoding    # includes Searchlight\n",
    "from nilearn.plotting import plot_design_matrix, plot_contrast_matrix, plot_stat_map, plot_roi\n",
    "from nilearn.glm.first_level import FirstLevelModel, compute_regressor, spm_hrf\n",
    "from nilearn.glm.thresholding import threshold_stats_img\n",
    "from nilearn.maskers import NiftiSpheresMasker\n",
    "\n",
    "from bids.layout import BIDSLayout # to fetch data from BIDS-compliant datasets\n",
    "\n",
    "import nibabel as nib # NiBabel, to read and write neuroimaging data, https://nipy.org/nibabel/\n",
    "\n",
    "# scikit-learn is the major library for machine learning in Python:\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing   # includes LabelEncoder, OneHotEncoder, StandardScaler...\n",
    "from sklearn import decomposition   # includes PCA\n",
    "from sklearn import model_selection # includes StratifiedKFold, LeaveOneGroupOut, etc....\n",
    "from sklearn import linear_model    # includes LogisticRegression, RidgeClassifier...\n",
    "from sklearn import svm             # includes SVC, NuSVC & LinearSCV...\n",
    "from sklearn import discriminant_analysis # includes LinearDiscriminantAnalysis\n",
    "from sklearn import metrics         # includes accuray, balanced accuracy, roc_auc_score, etc....\n",
    "from sklearn import pipeline        # includes make_pipeline\n",
    "from sklearn import inspection      # includes DecisionBoundaryDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wd = '/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/' # <-- CHANGE TO YOURS\n",
    "os.chdir(wd)\n",
    "#print(f\"Working directory currently {os.getcwd()}\")\n",
    "\n",
    "# Make new directory for output\n",
    "output = os.path.join(wd, 'mvpa')\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "print(f\"Output directory {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data_dir = 'data' # data in BIDS format\n",
    "fmri_results_dir = 'results' # results in BIDS format\n",
    "\n",
    "# --- Set up the BIDS layout\n",
    "layout = BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "# Attach the results folder to the layout. \n",
    "layout.add_derivatives(os.path.join(fmri_results_dir, \"first-level\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 preprocessed functional files\n",
      "Found 9 event files\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-01_LSA_2FF_2SF_betas.nii.gz\n"
     ]
    }
   ],
   "source": [
    "sID = '15'\n",
    "\n",
    "bold = layout.get(subject = sID, datatype = 'func', desc = 'preproc', extension = '.nii.gz', return_type = 'filename')\n",
    "print(\"Found \" + str(len(bold)) + \" preprocessed functional files\")\n",
    "\n",
    "events_files = layout.get(subject=sID, datatype='func', suffix='events', extension=\".tsv\", return_type='filename')\n",
    "print(\"Found \" + str(len(events_files)) + \" event files\")\n",
    "\n",
    "model_name = 'LSA_2FF_2SF' \n",
    "outdir = os.path.join(output, model_name, 'sub-' + sID)\n",
    "\n",
    "conditions = ['IniFF', 'ImmFF', 'DelFF', 'IniUF', 'ImmUF', 'DelUF', 'IniSF', 'ImmSF', 'DelSF']\n",
    "conditions_of_interest = ['IniFF', 'DelFF', 'IniSF', 'DelSF']\n",
    "other_conditions = [val for val in conditions if val not in conditions_of_interest]\n",
    "\n",
    "lsa_events = []\n",
    "for run, events_file in enumerate(events_files):\n",
    "    events_df = pd.read_table(events_file)\n",
    "    for j, event in enumerate(events_df['trial_type']):\n",
    "        if event in conditions_of_interest:\n",
    "            events_df.loc[j, 'trial_type'] = event + events_df['stim_file'][j][-8:-4]\n",
    "    lsa_events.append(events_df)\n",
    "\n",
    "img_4d_fname = []\n",
    "trial_labels = []\n",
    "for run in range(len(bold)):\n",
    "    img_4d_fname.append(os.path.join(outdir, os.path.basename(bold[run]).split(\"space\")[0] + model_name + '_betas.nii.gz'))\n",
    "    trials_to_save = lsa_events[run][\"trial_type\"]\n",
    "    trials_to_save = [val for val in trials_to_save if val not in other_conditions]\n",
    "    trials_to_save = [label[3:5] for label in trials_to_save]\n",
    "    trial_labels.extend(trials_to_save)\n",
    "\n",
    "print(img_4d_fname[0])\n",
    "#print(labels_per_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI to ROI\n",
    "\n",
    "Let's define again the functional fusiform ROI and the anatomical amygdala ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 76, 64)\n",
      "(97, 115, 97)\n"
     ]
    }
   ],
   "source": [
    "amyg_ROI = nib.load(os.path.join(fmri_results_dir,'amygdala_mask.nii.gz'))\n",
    "print(amyg_ROI.get_fdata().shape)\n",
    "amyg_ROI = nl.image.resample_to_img(amyg_ROI, img_4d_fname[0], interpolation='nearest') # resample ROI to bold resolution to save time when estimating glm below\n",
    "print(amyg_ROI.get_fdata().shape)\n",
    "\n",
    "fusi_ROI = nib.load(os.path.join(fmri_results_dir,'sphere_and_faces-scrambled_Zmap_fdr.nii.gz'))\n",
    "# shouldn't need to resample fusi ROI, but just to be safe!\n",
    "fusi_ROI = nl.image.resample_to_img(fusi_ROI, img_4d_fname[0], interpolation='nearest') \n",
    "\n",
    "amyg_masker = nl.input_data.NiftiMasker(amyg_ROI)\n",
    "fusi_masker = nl.input_data.NiftiMasker(fusi_ROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-01_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-02_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-03_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-04_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-05_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-06_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-07_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-08_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-09_LSA_2FF_2SF_betas.nii.gz\n"
     ]
    }
   ],
   "source": [
    "fusi_data_per_run = []\n",
    "for run in range(len(img_4d_fname)):\n",
    "    print(img_4d_fname[run])\n",
    "    fusi_data = nl.image.load_img(img_4d_fname[run]) \n",
    "    fusi_data = fusi_masker.fit_transform(img_4d_fname[run])\n",
    "    fusi_data = np.mean(fusi_data, axis=1) # average across voxels\n",
    "    fusi_data_per_run.append(fusi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 FF trials and 202 SF trials\n"
     ]
    }
   ],
   "source": [
    "fusi_data = np.concatenate(fusi_data_per_run, axis=0)\n",
    "\n",
    "con1 = [i for i, x in enumerate(trial_labels) if x == 'FF']\n",
    "con1 = fusi_data[con1]\n",
    "con2 = [i for i, x in enumerate(trial_labels) if x == 'SF']\n",
    "con2 = fusi_data[con2]\n",
    "print(f'{len(con1)} FF trials and {len(con2)} SF trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "print(len(con1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "univariate connectivity - multivariate connectivity do exist, where do not average across voxels (review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(appended_beta_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = skl.svm.LinearSVC(dual=True)\n",
    "\n",
    "scaler = skl.preprocessing.StandardScaler()\n",
    "\n",
    "pipe = skl.pipeline.make_pipeline(scaler, SVM)\n",
    "\n",
    "n_per_sub = 2\n",
    "sub_indices =  np.concatenate([[i] * 2 * n_per_sub for i in range(16)]) # get the indices of each run\n",
    "print(\"Sub indices:\",sub_indices,\"\\n\")\n",
    "print(concatenated_labels)\n",
    "\n",
    "logo = skl.model_selection.LeaveOneGroupOut()\n",
    "\n",
    "accuracy = skl.model_selection.cross_val_score(pipe, concatenated_beta_maps, concatenated_labels,\n",
    "                                    groups  = sub_indices,\n",
    "                                    scoring = 'accuracy',\n",
    "                                    cv      = logo)\n",
    "print(\"Crossvalidated accuracy per fold:\", accuracy)                   \n",
    "print(\"Mean accuracy:\", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting trial-specific responses (Betas)\n",
    "\n",
    "We are going to try to classify famous versus unfamiliar faces. \n",
    "\n",
    "We first need to extract estimates for every trial that contained a famous or unfamiliar faces, restricting ourselves to initial presentations.\n",
    "\n",
    "One way - so-called **Least-Squares-All** (LSA) simply involves creating a new regressor for each trial (i.e, using a single event onset). This can be appropriate in some situations, but generally results in quite noisy Beta estimates. An alternative is **Least-Squares-Separate** (LSS) which \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data_dir = 'data' # data in BIDS format\n",
    "fmri_results_dir = 'results' # results in BIDS format\n",
    "\n",
    "# --- Set up the BIDS layout\n",
    "layout = BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "\n",
    "# Attach the results folder to the layout. \n",
    "layout.add_derivatives(os.path.join(fmri_results_dir, \"first-level\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sID = '15'\n",
    "bold = layout.get(subject = sID, datatype = 'func', desc = 'preproc', extension = '.nii.gz', return_type = 'filename')\n",
    "print(\"Found \" + str(len(bold)) + \" preprocessed functional files\")\n",
    "print(bold[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_files = layout.get(subject=sID, datatype='func', suffix='events', extension=\".tsv\", return_type='filename')\n",
    "print(\"Found \" + str(len(events_files)) + \" event files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['IniFF', 'ImmFF', 'DelFF', 'IniUF', 'ImmUF', 'DelUF', 'IniSF', 'ImmSF', 'DelSF']\n",
    "conditions_of_interest = ['IniFF', 'ImmFF', 'DelFF', 'IniUF', 'ImmUF', 'DelUF']\n",
    "#conditions_of_interest = ['IniFF', 'IniUF']\n",
    "#conditions_of_interest = ['IniFF', 'IniSF']\n",
    "other_conditions = [val for val in conditions if val not in conditions_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_events = []\n",
    "for run, events_file in enumerate(events_files):\n",
    "    events_df = pd.read_table(events_file)\n",
    "    events_df = events_df.drop(columns = ['button_pushed', 'stim_file', 'trigger', 'circle_duration', 'response_time'])\n",
    "    lsa_events.append(events_df)\n",
    "\n",
    "print(lsa_events[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confounds of interest\n",
    "confound_files = layout.get(subject = sID, datatype = 'func', desc = 'confounds', extension = \".tsv\", return_type = 'filename')\n",
    "relevant_confounds = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "confounds_for_glm = []\n",
    "for conf_file in confound_files:\n",
    "    this_conf = pd.read_table(conf_file)\n",
    "    conf_subset = this_conf[relevant_confounds].fillna(0) # replace NaN with 0\n",
    "    confounds_for_glm.append(conf_subset)\n",
    "\n",
    "TR = layout.get_tr()\n",
    "print('TR:', TR)\n",
    "slice_timing = layout.get_metadata(bold[0])\n",
    "if slice_timing['SliceTimingCorrected']:\n",
    "  slice_time_ref = slice_timing['StartTime'] / TR\n",
    "  print('Slice timing reference:', slice_time_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - no smoothing and no signal scaling; see https://nilearn.github.io/dev/auto_examples/07_advanced/plot_beta_series.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = FirstLevelModel(t_r = TR,\n",
    "                           slice_time_ref = slice_time_ref,\n",
    "                           hrf_model = 'spm',\n",
    "                           drift_model = 'cosine',\n",
    "                           drift_order = 1,\n",
    "                           high_pass = 0.01,\n",
    "                           smoothing_fwhm = None,\n",
    "                           signal_scaling = (0, 1), # grand mean scaling only\n",
    "                           noise_model = 'ar1',\n",
    "#                           mask_img = roi_mask,\n",
    "                           memory = 'scratch',\n",
    "                           minimize_memory = False);\n",
    "# fit for run 1\n",
    "#run_glm = fmri_glm.fit(bold[0], lsa_events[0], confounds_for_glm[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrices = fmri_glm.design_matrices_\n",
    "print('Design matrix for run', 1)\n",
    "plot_design_matrix(design_matrices[0], output_file=None)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run through all 9 runs, fitting the GLM, calculating the effect size (beta) for each initial FF and UF trial (to save in a 4D image for each run) and also defining the labels associated with each trial, which we will need for classification later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LSU' # need to add LSS\n",
    "outdir = os.path.join(output, model_name, 'sub-' + sID)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "affine = nib.load(bold[0]).affine\n",
    "\n",
    "lab_enc = skl.preprocessing.LabelEncoder() # this initialises the LabelEncoder object\n",
    "\n",
    "img_4d_fname = []\n",
    "labels_per_run = []\n",
    "for run in range(len(bold)):\n",
    "    # fit GLM\n",
    "    run_glm = fmri_glm.fit(bold[run], lsa_events[run], confounds_for_glm[run]);\n",
    "    \n",
    "    # Compute betas for each initial FF and UF trial\n",
    "    trials_to_save = conditions_of_interest\n",
    "    lsa_beta_maps  = [] \n",
    "    print(f'Run {run}: Creating {len(trials_to_save)} beta images...')\n",
    "    for trial in trials_to_save:\n",
    "        beta_map = run_glm.compute_contrast(trial, output_type = 'effect_size');\n",
    "        #if want to save each trial as a separate beta image\n",
    "        #beta_map.to_filename(os.path.join(outdir, os.path.basename(bold[0]).split(\"space\")[0] + 'desc-' + trial + '_beta.nii.gz'))\n",
    "        lsa_beta_maps.append(beta_map)\n",
    "\n",
    "    # save trials as a 4D image so can load later\n",
    "    print(f'Run {run+1}: Saving to a 4D file...')\n",
    "    volumes = np.squeeze([lsa_beta_maps[trial].get_fdata() for trial in range(0,len(lsa_beta_maps))])\n",
    "    data_4d = np.stack(volumes, axis=-1)  # Stack along the 4th dimension\n",
    "    img_4d  = nib.Nifti1Image(data_4d, affine);\n",
    "    img_4d_fname.append(os.path.join(outdir, os.path.basename(bold[run]).split(\"space\")[0] + 'LSA_betas.nii.gz'))\n",
    "    nib.save(img_4d, img_4d_fname[run])\n",
    "\n",
    "    # calculate labels associated with each trial\n",
    "    trials_to_save = [label[3:5] for label in trials_to_save]\n",
    "    labels_per_run.append(lab_enc.fit_transform(trials_to_save)) # Encode as integers\n",
    " \n",
    "# Remove the cached directory\n",
    "!rm -rf scratch/joblib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_per_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amyg_ROI = nib.load('amygdala_mask.nii.gz')\n",
    "print(amyg_ROI.get_fdata().shape)\n",
    "amyg_ROI = nl.image.resample_to_img(amyg_ROI, img_4d_fname[0], interpolation='nearest') # resample ROI to bold resolution to save time when estimating glm below\n",
    "print(amyg_ROI.get_fdata().shape)\n",
    "\n",
    "fusi_ROI = nib.load('sphere_mask.nii.gz')\n",
    "print(fusi_ROI.get_fdata().shape)\n",
    "fusi_ROI = nl.image.resample_to_img(fusi_ROI, img_4d_fname[0], interpolation='nearest') # resample ROI to bold resolution to save time when estimating glm below\n",
    "print(fusi_ROI.get_fdata().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We don't expect all of these voxels to contain signal (some are not even in the brain!). So we will **select voxels from a Region Of Interest (ROI).** This is a type of **feature selection**, which restricts the analysis to brain regions we are interested in, and/or brain regions where we expect to find signal. To avoid biasing the results, any data used for feature selection must be independent of the data being classified. Since these data are in MNI space, we can use any mask independently defined in MNI space. Here, we'll use Nilearn to load anatomical ROIs from the Harvard-Oxford subcortical atlas. These are returned as a dictionary that stores the `filename`, `description` and, importantly, `labels` (ROI names) and `maps` (a 3D volume of integers that index the ROI names):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass this ROI to a `masker` object, and apply it to the activation patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = nl.input_data.NiftiMasker(fusi_ROI)\n",
    "\n",
    "patterns_4D = nl.image.load_img(img_4d_fname[0])\n",
    "patterns_ROI = masker.fit_transform(patterns_4D) \n",
    "\n",
    "print('Shape of pattern matrix from chosen ROI:', patterns_ROI.shape)\n",
    "patterns_ROI = patterns_ROI[:,~np.all(patterns_ROI == 0, axis = 0)]\n",
    "print('Shape of pattern matrix from chosen ROI:', patterns_ROI.shape)\n",
    "\n",
    "ax = sns.heatmap(patterns_ROI, cmap='coolwarm', center=0, robust=1, xticklabels=50, yticklabels=10)\n",
    "ax.set(xlabel=\"voxels\", ylabel=\"samples / patterns / faces\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that features are typically standardized before PCA, otherwise the principal components will be dominated by features with larger scales. However, standardization means that features that originally had low variance will be upweighted, under the assumption that all features are expected to be equally predictive. In fMRI, noisy/non-predictive features may have low variance, so upweighting them may be detrimental to performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now let's put everything together: load data from all six runs of this day..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_data_per_run   = [] # list of ROI's pattern matrices (one for each run)\n",
    "brain_data_per_run = [] # also save the whole brain data for running a searchlight later\n",
    "\n",
    "for run in range(len(bold)):\n",
    "    print(img_4d_fname[run])\n",
    "    patterns_4D = nl.image.load_img(img_4d_fname[run]) \n",
    "    #patterns_4D = nl.image.clean_img(patterns_4D, standardize=True, detrend=False)\n",
    "    patterns_ROI = masker.fit_transform(patterns_4D) \n",
    "\n",
    "    patterns_ROI = patterns_ROI[[0,3],:]\n",
    "\n",
    "    ROI_data_per_run.append( patterns_ROI )  # append this run's data to list\n",
    "    brain_data_per_run.append( patterns_4D ) # whole-brain data that will use later in search light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "...and try to decode expression within the amygdala ROI, using leave-one-run-out cross-validatation. This time we'll use yet another classifier - a Fisher's linear discriminant classifer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data    = np.concatenate(ROI_data_per_run, axis=0) # concatenate data across runs\n",
    "print(all_data.shape)\n",
    "all_data = all_data[:,~np.any(all_data== 0, axis = 0)] # remove any voxels with 0\n",
    "print(all_data.shape)\n",
    "\n",
    "nruns=(len(ROI_data_per_run))\n",
    "npatterns=(np.size(ROI_data_per_run[0],axis=0))\n",
    "print(npatterns)\n",
    "run_indices = np.concatenate([[i] * npatterns for i in range(nruns)])\n",
    "print(run_indices)\n",
    "\n",
    "all_labels = np.concatenate([[0, 1] for i in range(nruns)])\n",
    "#all_labels  = np.concatenate(labels_per_run, axis=0) # concatenate labels across runs\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA  = skl.discriminant_analysis.LinearDiscriminantAnalysis();\n",
    "pipe = pipeline.make_pipeline(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = skl.svm.LinearSVC(dual=True)\n",
    "pipe = pipeline.make_pipeline(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = skl.model_selection.LeaveOneGroupOut()\n",
    "fold_score = skl.model_selection.cross_val_score(pipe, all_data, all_labels, groups = run_indices,\n",
    "                                    scoring = 'roc_auc', cv = logo)\n",
    "observed_score = np.mean(fold_score)\n",
    "print(\"Cross-validated score per fold:\", fold_score)  \n",
    "print(\"Mean cross-validated score:  \", observed_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance testing\n",
    "\n",
    "This is only just above chance. Is it significantly above chance? (See Rik's lectures.) We cannot test this using a t-test across folds, because the folds are not independent (they share training data). If we had multiple participants we could use a **one-sample t-test across participants**, versus chance. This is valid (assuming the specified chance level is correct) because participants are independent. For \"within-context\" cross-validation, below-chance accuracy is not meaningful (the true value can't be negative), which means we can use a one-tailed test; it also means that the test only provides fixed-effects rather than random-effects inference (see Allefeld et al., 2016). For cross-validation that generalises *across* contexts (e.g. train the classifer to decode the expression of younger faces, then test decoding of the expression of older faces) negative performance *could* be meaningful, so random-effects inference is justified. Similarly, when comparing classfication performance across conditions (e.g. is the performance of expression decoding for younger faces different from that for older faces?), paired-tests or repeated-measures ANOVA allow random-effects inference.\n",
    "\n",
    "If we want **to assess significance for a single participant, we would need a permutation test**. (Actually this applies to testing the significance of any single *classification* that can't be repeated across independent samples, e.g. a single classification of the disease status of multiple partarticipants.) The permutation test involves shuffling the labels many times (e.g. 10,000) to generate a null distribution of classifier performance that would be expected under the null hypothesis of no difference between classes.\n",
    "\n",
    "In some situations, permutation tests may be necessary, but they do have disadvantages:\n",
    " - the large number of required permutations makes them slow\n",
    " - low numbers of samples limit the number of unique permutations, which limits robustness of the p-value\n",
    " - there's some complexity in ensuring the labels are actually \"exchangeable\"\n",
    "\n",
    "First we'll code this explicitly within the cross-validation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = tuple(logo.split(all_data, all_labels, run_indices))\n",
    "\n",
    "n_permutations = 200 # this is not nearly enough, but it will still take a while\n",
    "permuted_scores = np.zeros(n_permutations)\n",
    "print(f'Permuting {n_permutations} times:')\n",
    "tic = time.time()\n",
    "for p in range(n_permutations):\n",
    "    \n",
    "    fold_score=np.full(nruns,np.nan)\n",
    "    for i, fold in enumerate(folds):\n",
    "        train_idx, test_idx = fold   \n",
    "\n",
    "        #### previously we trained the classifier like this:\n",
    "        # pipe.fit( all_data[train_idx,:], all_labels[train_idx] )\n",
    "        #### now we want to do something like this:\n",
    "        # pipe.fit( all_data[train_idx,:], np.random.shuffle(all_labels[train_idx]) )\n",
    "        #### but confusingly, we need to break it up like this:\n",
    "        train_labels = all_labels[train_idx] \n",
    "        np.random.shuffle(train_labels)\n",
    "        pipe.fit(all_data[train_idx,:], train_labels)\n",
    "        #### because indexing makes a temporary copy, and shuffle operates in-place\n",
    "\n",
    "        predicted_probabilities = pipe.predict_proba(all_data[test_idx,:]) # do prediction on test data\n",
    "        fold_score[i] = skl.metrics.roc_auc_score(all_labels[test_idx], predicted_probabilities[:,1]) # syntax for binary classifcation\n",
    "    permuted_scores[p] = np.mean(fold_score)\n",
    "    print('.',end='')\n",
    "print('Done.')\n",
    "print('Took ', time.time()-tic,' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the observed score and compare it to the null distribution of scores from permuted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(permuted_scores, element='step', alpha=0.5)\n",
    "ax.set(xlabel = 'AUROC of permuted data');\n",
    "lh = ax.axvline(np.percentile(permuted_scores, 95), color='b',label='95% threshold')\n",
    "mh = ax.plot(observed_score, 0, marker='o', color='r', markersize=10, label='observed score')\n",
    "ax.legend();\n",
    "\n",
    "p = (sum(permuted_scores > observed_score) + 1) / (n_permutations + 1)\n",
    "print('p vlaue = ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there is no evidence for significant classification of expression within this ROI.\n",
    "\n",
    "\\\n",
    "Just as scikit-learn has a function (`cross_val_score`) to simplify the cross-validation loop, it also has a similar function (`permutation_test_score`) to simplify the permutation process. This also allows the permutation to be run in parallel, which can be much faster! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "actual_score, permuted_scores, p = skl.model_selection.permutation_test_score(pipe, all_data, all_labels, groups = run_indices,\n",
    "                                    scoring = 'roc_auc', cv = logo, n_permutations = n_permutations, n_jobs = -1, random_state = None)\n",
    "\n",
    "print('Took ', time.time()-tic,' s')\n",
    "print(\"p value = \", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This p-value should, on average, be similar to above, but will differ because of the randomness of the permutations. As the number of unique permutations increases, the p-values from different repetitions should become more similar, giving an idea of the robustness of the permutation-p-value.\n",
    "\n",
    "\\\n",
    "So far we have considered a single ROI. If we want to know where in the brain a representation is strongest, we can compare multiple ROIs. Sometimes we might be interested in discrete regions that cover the brain (e.g. an atlas/parcellation), but if we want a spatially continuous output, without making assumptions about area borders, we can use a \"searchlight\". \n",
    "\n",
    "## Searchlight analysis\n",
    "\n",
    "A searchlight is not really anything special. It's just a set of overlapping ROIs, often spherical, that cover the brain (or analysis mask of interest). Each searchlight's classification performance is typically assigned to its central voxel. The inferences one would make are very similar to a voxel-wise, mass-univariate analysis of smoothed data (see Dace's sessions). Increasing the size of the searchlight reduces the spatial specificity of the inference, while tending to increase sensitivity, in a very similar way to increasing the smoothing kernal in univariate analysis. Similarly to the matched-filter theorem for univariate analyses, the optimal searchlight size will depend on the spatial scale of the signal and the spatial scale of the noise. It is sometimes assumed that searchlights need to be spatially contiguous, because they usually are, but the definition of a searchlight is just as flexible as the definition of any ROI. \n",
    "\n",
    "A searchlight analysis can be set up using nilearn's `.searchlight` object. This *is* restricted to contiguous spherical searchlights. When the searchlight object is created, it needs to be given a mask specifying which voxels will be included in the searchlights. So first we'll get an analysis mask that covers the whole brain. As before, we can load a template-space mask from nilearn, and resample it to match the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_mask = nl.datasets.load_mni152_brain_mask()\n",
    "brain_mask = nl.image.resample_to_img(brain_mask, patterns_4D, interpolation='nearest')\n",
    "\n",
    "has_data =   nl.image.math_img('(img.prod(axis=3) !=0)', img=patterns_4D); # not zero for any sample\n",
    "brain_mask = nl.image.math_img('(in_mask & has_data)', in_mask=brain_mask, has_data=has_data);\n",
    "\n",
    "display = nl.plotting.plot_roi(brain_mask,alpha=0.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We can also provide a second, optional mask that specifies where the searchlights are centred. To save some time, we'll constrain these to be in cortical grey matter of the left hemisphere. We could also load a grey-matter mask from nilearn, but here we'll use the 'Left Cerebral Cortex' region of the atlas we loaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gm_mask = nl.datasets.load_mni152_gm_mask()\n",
    "# gm_mask = nl.image.resample_to_img(gm_mask, patterns_4D, interpolation='nearest')\n",
    "\n",
    "l_cortex_id = HO_atlas['labels'].index('Left Cerebral Cortex')                                # get the ID of the Left Cerebral Cortex\n",
    "gm_mask     = nl.image.math_img(f'(map == {l_cortex_id} )', map=resampled_ROI_map)           # create binary mask of this ROI\n",
    "gm_mask     = nl.image.math_img('(in_mask & has_data)', in_mask=gm_mask, has_data=has_data); # restrict mask to voxels that have data\n",
    "\n",
    "display = nl.plotting.plot_roi(nl.image.math_img('a+2*b', a=gm_mask, b=brain_mask),alpha=0.6,cmap='summer');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the `searchlight` object. We will also specify some other optional inputs, explained in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = skl.pipeline.make_pipeline(LDA) # we won't do PCA, because this would fail for some searchlights (think about why)\n",
    "\n",
    "sl = nl.decoding.SearchLight(\n",
    "    mask_img = brain_mask,       # only include these voxels within searchlights\n",
    "    process_mask_img = gm_mask,  # only centre searchlights on these voxels \n",
    "    radius = 5,                  # in mm\n",
    "    estimator = pipe,            # a classifier or pipeline object\n",
    "    n_jobs = -1,                 # how many CPUs to use (-1 means the maximum available)\n",
    "    scoring = 'roc_auc',         # choice of scoring metric\n",
    "    cv = logo,                   # cross-validation object\n",
    "    verbose = False              # this is slow, so if running locally (1 job) set it to true to know how far it's got; when running in parallel it produces too much output, so set to false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "To launch the searchlight analysis, call the `searchlight` object's `.fit` method, passing it the patterns, the labels, and optionally the groups for cross-validation.\n",
    "This will take a while (probably about 5 minutes on the virtual machine, with 4 cores)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = nl.image.concat_imgs(brain_data_per_run) # concatenate 4D nifti files from each run (along the 4th dimension)\n",
    "\n",
    "print('Shape of 4D nifti data:', all_data.shape)\n",
    "print('Shape of mask:', sl.mask_img.shape)\n",
    "print('Shape of process_mask:', sl.process_mask_img.shape)\n",
    "\n",
    "print('Running searchlight...')\n",
    "tic = time.time()\n",
    "sl.fit(all_data,all_labels, groups=run_indices)\n",
    "print('Took ', time.time()-tic,' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "The output (here, mean AUROC) per voxel is stored in the  `scores_` property of the searchlight object (as a 3D numpy array). If we convert this to a nifti volume, we can plot it using nilearn. We'll also subtract chance (here, 0.5), so that positive and negative values will indicate above- and below-chance decoding performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of output numpy array: ', sl.scores_.shape)\n",
    "scores = sl.scores_ - 0.5 # subtract chance\n",
    "score_img = nl.image.new_img_like(brain_mask, sl.scores_ - 0.5); # save in samme format as the brain_mask\n",
    "score_img = nl.image.math_img('score_img * processing_mask', score_img=score_img,processing_mask=gm_mask); # set voxels outside the processing mask to zero\n",
    "nl.plotting.plot_stat_map(score_img, threshold=0, cmap='bwr', cut_coords=(-18, -31, -16));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.plotting.view_img_on_surf(score_img, threshold=0,cmap='bwr',symmetric_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
