{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "- **Created:** November 2024 by [Rik Henson](https://www.mrc-cbu.cam.ac.uk/people/rik.henson/) with thanks to [Petar Raykov](https://www.mrc-cbu.cam.ac.uk/people/Petar.Raykov)\n",
    "- **conda environment**: Needs the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts.\n",
    "\n",
    "# Functional Connectivity\n",
    "\n",
    "This notebook introduces two types of fMRI connectivity (for a single subject):\n",
    "\n",
    "1. Task-based \"beta-series regression\" (BSR) to test whether the correlation between single-trial BOLD responses between two regions/voxels differs as a function of trial-type (condition). This starts with BSR between two ROIs (using NiftiMaskers from previous notebooks) and then shows BSR between a seed ROI and all other voxels in an image.\n",
    "\n",
    "2. State-based \"time-series correlation\" (TSC) across TRs between ROIs, having removed confounds like motion artifacts, WM/CSF signal, global signal etc. Normally this type of analysis is done on resting-state data (or data from continuous stimuli like movie watching), but there are no such data in the dataset we are using. Nonetheless, we can simulate resting-state data by removing task effects (i.e., treating the task regressors in our design matrix as another confound). The notebook finishes with creation of a \"functional connectome\" of connectivity between every pair of ROIs in an atlas, which is the type of connectivity matrix used in the next notebook on network analysis.\n",
    "\n",
    "It does not yet demonstrate \"psychophysiological interactions\" (PPIs) - a third type of connectivity in which a model is created with a timeseries, a task regressor and the interaction between these two - which I hope to add in future. \n",
    "\n",
    "Note that there are also many powerful data-driven methods (like ICA) that can be used to estimate function networks of regions with similar timeseries (see for example [Smitha et al (2017)](https://pmc.ncbi.nlm.nih.gov/articles/PMC5524274) for example review), as well as complex dynamical models (like DCM or HMMs) that get closer to measuring true \"effective connectivity\" (by simulating a full network of regions, see [Stephan & Friston (2011)](https://pmc.ncbi.nlm.nih.gov/articles/PMC3013343/) for example) - but these are not covered here.\n",
    "\n",
    "Finally, we are using unidimensional connectivity, averaging values across voxels within each ROI, though there are also multidimensional ways to estimate connectivity (see [Basti et al (2020)](https://pubmed.ncbi.nlm.nih.gov/32682988/) for example review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 0. Getting Ready\n",
    "\n",
    "As usual, we need some python packages like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # plotting\n",
    "\n",
    "import os           # To interact with the operating system, including files and paths (e.g. path.join)\n",
    "import bids.layout  # To fetch data from BIDS-compliant datasets\n",
    "import numpy        # This lets python process arrays/matrices\n",
    "import pandas       # To use \"dataframes\"      \n",
    "import nibabel      # Basic nifti image utilities\n",
    "\n",
    "import nilearn                  # Many useful functions for MRI, including...\n",
    "from nilearn import image       # to load (load_img), resample (resample_to_img), manipulate (math_img) fMRI data, etc.\n",
    "from nilearn import input_data  # includes NiftiMasker, NiftiLabelsMasker\n",
    "from nilearn.maskers import NiftiMasker, NiftiSpheresMasker # For extracting data from images\n",
    "from nilearn import plotting    # includes plot_roi, plot_stat_map, view_img_on_surf, etc.\n",
    "from nilearn import datasets    # for atlases below\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "import scipy                     # statistical tools\n",
    "from scipy.stats import pearsonr # like Pearson correlation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set-up our input and output directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory currently /mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/results/fcon\n"
     ]
    }
   ],
   "source": [
    "wd = '/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/' # <-- CHANGE TO YOURS\n",
    "out_dir = os.path.join(wd, 'results', 'fcon')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "os.chdir(out_dir)\n",
    "print(f\"Working directory currently {os.getcwd()}\")\n",
    "\n",
    "fmri_data_dir = os.path.join(wd, 'data') # data in BIDS format\n",
    "fmri_results_dir = os.path.join(wd, 'results') # results in BIDS format\n",
    "\n",
    "layout = bids.layout.BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "layout.add_derivatives(os.path.join(fmri_results_dir, \"first-level\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task-based Beta-series regression\n",
    "\n",
    "We are going to re-use the trial onsets and single-trial Betas from the previous notebook. For reasons given in that notebook, we are going to focus on Initial (Ini) and Delayed (Del) presentations of Famous Faces (FF) and Scrambled Faces (SF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 event files\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-01_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-02_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-03_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-04_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-05_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-06_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-07_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-08_LSA_2FF_2SF_betas.nii.gz\n",
      "/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-09_LSA_2FF_2SF_betas.nii.gz\n",
      "Found 9 4D files, each with single-trial images for a run\n"
     ]
    }
   ],
   "source": [
    "sID = '15' # same subject we used before\n",
    "\n",
    "events_files = layout.get(subject=sID, datatype='func', suffix='events', extension=\".tsv\", return_type='filename')\n",
    "print(\"Found \" + str(len(events_files)) + \" event files\")\n",
    "\n",
    "conditions = ['IniFF', 'ImmFF', 'DelFF', 'IniUF', 'ImmUF', 'DelUF', 'IniSF', 'ImmSF', 'DelSF']\n",
    "conditions_of_interest = ['IniFF', 'DelFF', 'IniSF', 'DelSF'] # see previous notebook for justification of focusing on these 4 trial-types\n",
    "other_conditions = [val for val in conditions if val not in conditions_of_interest]\n",
    "\n",
    "mvpa_dir = os.path.join(wd, 'mvpa') # where the beta images were saved from MVPA notebook, which we need for beta series regression\n",
    "model_name = 'LSA_2FF_2SF' \n",
    "outdir = os.path.join(mvpa_dir, model_name, 'sub-' + sID)\n",
    "\n",
    "beta_filenames = [] # name of 4D files that contain Beta estimates for single trials for each run\n",
    "trial_labels = []\n",
    "for run, events_file in enumerate(events_files):\n",
    "    beta_file = os.path.join(outdir, os.path.basename(events_file).split(\"events\")[0] + model_name + '_betas.nii.gz')\n",
    "    print(beta_file)\n",
    "    beta_filenames.append(beta_file)\n",
    "\n",
    "    events_df = pandas.read_table(events_file)\n",
    "    trials_to_save = events_df[\"trial_type\"]\n",
    "    trials_to_save = [val for val in trials_to_save if val not in other_conditions]\n",
    "    trials_to_save = [label[3:5] for label in trials_to_save]\n",
    "    trial_labels.extend(trials_to_save)\n",
    "\n",
    "print(\"Found \" + str(len(beta_filenames)) + \" 4D files, each with single-trial images for a run\")\n",
    "#print(labels_per_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ROI to ROI\n",
    "\n",
    "We will start by estimating connectivity between just two ROIs: the functional fusiform ROI and the anatomical amygdala ROI from previous notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 76, 64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File not found: '/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-01_LSA_2FF_2SF_betas.nii.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m amyg_ROI \u001b[38;5;241m=\u001b[39m nibabel\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fmri_results_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamygdala_mask.nii.gz\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(amyg_ROI\u001b[38;5;241m.\u001b[39mget_fdata()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m amyg_ROI \u001b[38;5;241m=\u001b[39m \u001b[43mnilearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample_to_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamyg_ROI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_filenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# resample ROI to bold resolution to save time when estimating glm below\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(amyg_ROI\u001b[38;5;241m.\u001b[39mget_fdata()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m fusi_ROI \u001b[38;5;241m=\u001b[39m nibabel\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fmri_results_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msphere_and_faces-scrambled_Zmap_fdr.nii.gz\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/mri/lib/python3.11/site-packages/nilearn/image/resampling.py:748\u001b[0m, in \u001b[0;36mresample_to_img\u001b[0;34m(source_img, target_img, interpolation, copy, order, clip, fill_value, force_resample)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresample_to_img\u001b[39m(\n\u001b[1;32m    688\u001b[0m     source_img,\n\u001b[1;32m    689\u001b[0m     target_img,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m     force_resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    696\u001b[0m ):\n\u001b[1;32m    697\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample a Niimg-like source image on a target Niimg-like image.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m    No registration is performed: the image should already be aligned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m \n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 748\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_niimg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m     target_shape \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# When target shape is greater than 3, we reduce to 3, to be compatible\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# with underlying call to resample_img\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mri/lib/python3.11/site-packages/nilearn/_utils/niimg_conversions.py:300\u001b[0m, in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mniimg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(niimg):\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mniimg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: File not found: '/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceProcessing/mvpa/LSA_2FF_2SF/sub-15/sub-15_ses-mri_task-facerecognition_run-01_LSA_2FF_2SF_betas.nii.gz'"
     ]
    }
   ],
   "source": [
    "amyg_ROI = nibabel.load(os.path.join(fmri_results_dir,'amygdala_mask.nii.gz'))\n",
    "print(amyg_ROI.get_fdata().shape)\n",
    "amyg_ROI = nilearn.image.resample_to_img(amyg_ROI, beta_filenames[0], interpolation='nearest') # resample ROI to bold resolution to save time when estimating glm below\n",
    "print(amyg_ROI.get_fdata().shape)\n",
    "\n",
    "fusi_ROI = nibabel.load(os.path.join(fmri_results_dir,'sphere_and_faces-scrambled_Zmap_fdr.nii.gz'))\n",
    "# shouldn't need to resample fusi ROI, but just to be safe!\n",
    "fusi_ROI = nilearn.image.resample_to_img(fusi_ROI, beta_filenames[0], interpolation='nearest') \n",
    "\n",
    "amyg_masker = nilearn.input_data.NiftiMasker(amyg_ROI)\n",
    "fusi_masker = nilearn.input_data.NiftiMasker(fusi_ROI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then concatenate the single-trial Betas for each ROI and average over voxels in that ROI (will take a short while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusi_data_per_run = []\n",
    "amyg_data_per_run = []\n",
    "for run in range(len(beta_filenames)):\n",
    "    print(beta_filenames[run])\n",
    "\n",
    "    fusi_data = fusi_masker.fit_transform(beta_filenames[run])\n",
    "    fusi_data = numpy.mean(fusi_data, axis=1) # average across voxels\n",
    "    fusi_data_per_run.append(fusi_data)\n",
    "\n",
    "    amyg_data = amyg_masker.fit_transform(beta_filenames[run])\n",
    "    amyg_data = numpy.mean(amyg_data, axis=1) # average across voxels\n",
    "    amyg_data_per_run.append(amyg_data)\n",
    "\n",
    "fusi_data = numpy.concatenate(fusi_data_per_run, axis=0)\n",
    "amyg_data = numpy.concatenate(amyg_data_per_run, axis=0)\n",
    "print(\"Found \" + str(fusi_data.shape[0]) + \" single-trials in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also concatenate the trial labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_per_condition = []\n",
    "conditions = list(set(trial_labels))\n",
    "for con, condition in enumerate(conditions):\n",
    "    trials_per_condition.append([i for i, x in enumerate(trial_labels) if x == condition])\n",
    "    print(f'{len(trials_per_condition[con])} {condition} trials')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the \"Beta series\" (a time-series of single-trial Betas) for each ROI, separately for each condition (SF and FF), as well as gather up the Pearson R value for correlation between ROIs for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14,6));\n",
    "\n",
    "Rval_data = []\n",
    "for ax in range(len(axs.flat)):\n",
    "    pcor = pearsonr(fusi_data[trials_per_condition[ax]], amyg_data[trials_per_condition[ax]])\n",
    "    Rval_data.append(pcor.statistic)\n",
    "    \n",
    "    axs[ax].plot(fusi_data[trials_per_condition[ax]]);\n",
    "    axs[ax].plot(amyg_data[trials_per_condition[ax]]);\n",
    "#    axs[ax].plot(fusi_data[trials_per_condition[ax]], amyg_data[trials_per_condition[ax]],'o');\n",
    "    axs[ax].set_xlabel('Trial');\n",
    "    axs[ax].set_ylabel('Beta (effect) (a.u.)');\n",
    "    axs[ax].legend(['Fusiform', 'Amygdala']);\n",
    "    axs[ax].set_title('Condition ' + conditions[ax] + \": R = \" + str(round(pcor.statistic,2)) + \", p = \" + str(round(pcor.pvalue,4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the Fusiform and Amygdala ROIs seem to be more functionally connected for Scrambled than Famous faces. To test whether this difference in correlations is significant, we can use a permutation test to estimate the probability of getting such a difference in R values under the null (see Notebook on basic statistics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate null distributions of R-values for each condition\n",
    "num_rand = int(1e4) # Number of randomisations\n",
    "Rval = numpy.empty([len(conditions),num_rand])\n",
    "for con, condition in enumerate(conditions):\n",
    "    fusi_samp = fusi_data[trials_per_condition[con]]\n",
    "    \n",
    "    for r in range(num_rand):\n",
    "        amyg_perm = numpy.random.permutation(amyg_data[trials_per_condition[con]])\n",
    "        pcor = pearsonr(fusi_samp, amyg_perm)\n",
    "        Rval[con,r] = pcor.statistic\n",
    "\n",
    "# create null distribution of difference and true difference\n",
    "Rval_dif_data = Rval_data[0] - Rval_data[1]\n",
    "Rval_dif_perm = Rval[0,:] - Rval[1,:]\n",
    "\n",
    "# plot histogram of null (permuted) and true\n",
    "pdf, bin_edges = numpy.histogram(Rval_dif_perm, bins=100, density=True)\n",
    "plt.figure(); plt.bar((bin_edges[:-1] + bin_edges[1:]) / 2, pdf, width=bin_edges[1] - bin_edges[0])\n",
    "plt.plot([Rval_dif_data, Rval_dif_data], [0, 1.1*numpy.max(pdf)], 'r-')\n",
    "plt.title('Distribution of difference in R values from permutation'); plt.legend(['True Diff', 'Perm. Diff.']);\n",
    "\n",
    "pval = (numpy.sum(numpy.abs(Rval_dif_perm) >= numpy.abs(Rval_dif_data)) + 1) / (2*num_rand + 1) # two-tailed p-value\n",
    "print(f'Permuted p-value for difference in R values being as big as {round(Rval_dif_data,2)} is {round(pval,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have a significant difference, with stronger correlation between ROIs for Scrambled than Famous faces. There is also a parametric way to compare R-values, using the Fisher transform, which we show below because we will use later to estimate voxel-wise BSR in feasible times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Z-values using Fisher transform (atanh)\n",
    "difZ = numpy.arctanh(Rval_data[0]) - numpy.arctanh(Rval_data[1]) # faces - scrambled\n",
    "# Fisher test given number of observations in each condition\n",
    "Zdif = difZ / numpy.sqrt( 1 / (len(trials_per_condition[0]) - 3) + 1 / (len(trials_per_condition[1]) - 3) ) \n",
    "#two-tailed\n",
    "pval = scipy.stats.norm.cdf(-abs(Zdif)) + 1-scipy.stats.norm.cdf(abs(Zdif))\n",
    "print(f'Permuted p-value for difference in Z-values being as big as {round(difZ,2)} is {round(pval,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the difference is unlikely to happen by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ROI to whole brain\n",
    "\n",
    "We can also map out BSR between a seed ROI/voxel and all other voxels in the brain. First, we first load up the whole-brain data, smoothing by 10mm to get cleaner maps (not necessary, but can improve statistics depending on spatial scale of BSR differences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_masker = NiftiMasker(smoothing_fwhm=10)\n",
    "\n",
    "brain_data_per_run = []\n",
    "for run in range(len(beta_filenames)):\n",
    "    print(beta_filenames[run])\n",
    "    brain_data = brain_masker.fit_transform(beta_filenames[run])\n",
    "    brain_data_per_run.append(brain_data)\n",
    "    \n",
    "brain_data = numpy.concatenate(brain_data_per_run, axis=0)\n",
    "print(brain_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the Pearson correlation for all voxels by taking the dot product of the vector of fusiform beta-series with the matrix of beta-series for all brain voxels, and scaling by the respective norms of these beta-series (which is equivalent to the Pearson R value). We do this for each condition separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = trials_per_condition[0] # conditions[0]\n",
    "brain_corrs_con1 = numpy.dot(fusi_data[indices].T, brain_data[indices,])  # Pearson correlation proportional to dot product \n",
    "brain_corrs_con1 = brain_corrs_con1 / (numpy.linalg.norm(fusi_data[indices]) * numpy.linalg.norm(brain_data[indices,], axis=0))\n",
    "#brain_corrs_con1 = numpy.arctanh(brain_corrs_con1) # Fisher Transform R value to Z value\n",
    "\n",
    "indices = trials_per_condition[1] # conditions[1]\n",
    "brain_corrs_con2 = numpy.dot(fusi_data[indices].T, brain_data[indices,])  # Pearson correlation proportional to dot product \n",
    "brain_corrs_con2 = brain_corrs_con2 / (numpy.linalg.norm(fusi_data[indices]) * numpy.linalg.norm(brain_data[indices,], axis=0))\n",
    "#brain_corrs_con2 = numpy.arctanh(brain_corrs_con2) # Fisher Transform R value to Z value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can do the Fisher test to see where differences in the correlations are significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difZ = numpy.arctanh(brain_corrs_con1) - numpy.arctanh(brain_corrs_con2) # Faces > Scrambled\n",
    "Zdif = difZ / numpy.sqrt( 1 / (len(trials_per_condition[0]) - 3) + 1 / (len(trials_per_condition[1]) - 3) ) # Fisher method for comparing two independent correlations\n",
    "pval = scipy.stats.norm.cdf(-abs(Zdif)) + 1-scipy.stats.norm.cdf(abs(Zdif)) #two-tailed\n",
    "\n",
    "# convert to negative log10 (see earlier notebook), multiplied by sign of difference, and then plot\n",
    "sval = numpy.sign(difZ)\n",
    "pval = sval * -numpy.log10(pval)\n",
    "\n",
    "brain_diff_con_img = brain_masker.inverse_transform(pval)\n",
    "fusi_coords = (41.5, -48.5, -18.5) # from nb06\n",
    "display = nilearn.plotting.plot_stat_map(brain_diff_con_img, cut_coords=fusi_coords, threshold = -numpy.log10(0.001),\n",
    "    title=\"Signed log10 p-values for greater Beta Series correlation from Fusiform ROI for \" + conditions[1] + \" than \" + conditions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blue clusters around the amygdala echo what we found in the ROI-to-ROI analysis above, while the red clusters in more posterior occipital regions indicate stronger correlation with the fusiform when famous faces are presented relative to scrambled faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State-based time-series correlation (from task residuals)\n",
    "\n",
    "Finally, we will do some analyses similar to what you might do for resting-state fMRI, ie correlations between the original fMRI timeseries. Unfortunately this dataset does not have a proper resting-state run, but we can estimate this by removing (as best as possible) the effects of the task. The resulting timeseries reflect whatever is happening during the times between trials (note that there are short periods of rest between blocks of trials in this design).\n",
    "\n",
    "First let's load the raw fMRI volumes for the current subject, and recreate their design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold = layout.get(subject = sID, datatype = 'func', desc = 'preproc', extension = '.nii.gz', return_type = 'filename')\n",
    "nvols = nibabel.load(bold[0]).shape[-1] # assume same for all runs\n",
    "print(f\"Found {nvols} preprocessed functional files (dfs) for each of {len(bold)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are three main types of non-neural noise in fMRI timeseries: motion-related, physiological artifacts (like breathing) and instrumental noise (like scanner drift). We address each of these in turn.\n",
    "\n",
    "### 2.1 Confounds and expansion of motion parameters\n",
    "\n",
    "As in the notebook on subject-level models, we will collect the confounds created by fMRIPrep, but this time we will use some more than just the motion paramerers, since we want to ensure that any correlation between voxel timeseries (TSC) does not owe to noise sources like bioryhthms (eg breathing) that are shared across many voxels (we want it to reflect primarily neural activity). (Note that these additional confounds are less important for the task-based analysis we did in previous notebooks, because they are unlikely to be phase-locked to the regressors in our design matrix, so just appeared in the residual error, whereas for TSC we have no design matrix and the Pearson correlation estimate of functional connectivity depends on phase-locking across voxels.)\n",
    "\n",
    "First we will load the 6 motion parameters, but this time take various expansions of them, like their squares and the difference between successive TRs. These are to capture nonlinear and time-lagged artifacts caused by motion (like spin-history effects for example). These are sometimes called the \"Sattherthwaite 24\" set. They are already created by fMRIPrep, but we recreate them below for clarity. \n",
    "\n",
    "We will also add signal recorded from the white-matter (WM) and cerebrospinal fluid (CSF) partitions created by fMRIPrep, to capture biorhythms that should not contain any neural activity (unless contaminated by partial grey-matter volumes): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confound_files = layout.get(subject = sID, datatype = 'func', desc = 'confounds', extension = \".tsv\", return_type = 'filename')\n",
    "motion_confounds = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "other_confounds  = ['csf', 'white_matter']\n",
    "\n",
    "confounds_per_run = []\n",
    "for conf_file in confound_files:\n",
    "    this_conf = pandas.read_table(conf_file)\n",
    "    motion_subset = this_conf[motion_confounds].fillna(0) # replace NaN with 0\n",
    "    motion_change = motion_subset.diff().fillna(0)\n",
    "    motion_change.columns = 'diff_' + motion_subset.columns\n",
    "    motion_square = motion_subset.pow(2)\n",
    "    motion_square.columns = 'sq_' + motion_subset.columns \n",
    "    motion_change_square = motion_change.pow(2)\n",
    "    motion_change_square.columns = 'sq_' + motion_change.columns\n",
    "    motion_24 = pandas.concat([motion_subset, motion_change, motion_square, motion_change_square], axis=1)\n",
    "    confounds = pandas.concat([motion_24, this_conf[other_confounds].fillna(0)], axis=1)\n",
    "    confounds = (confounds - confounds.mean())/(confounds.std()) # Z-score just for visualisation\n",
    "    confounds_per_run.append(confounds)\n",
    "    \n",
    "print(f\"Using around {confounds_per_run[0].shape[1]} dfs per run to remove motion, CSF and WM effects\")\n",
    "confounds_per_run[-1].head() # show last run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Global Signal\n",
    "\n",
    "Another potential signature of physiological noise is the \"global signal\", the average timeseries over all brain voxels. Such \"global signal regression\" is contentious however: while it can be quite effective at removing noise, it can also remove true neural signal that happens to occur in many voxels. It also produces many negative Pearson correlation coefficients, since some pairs of voxels will be correlated, but less than the correlation between any voxel and the average. Negative correlations might be interpretable in terms of less-than-average connectivity (or true neural anti-correlation), but some argue that we should focus on positive correlations only (eg for graph-theoretic analysis in next notebook).\n",
    "\n",
    "Again, global signla is in the confounds file fromy fMRIprep, but we will recreate to show how it is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_images = nilearn.image.load_img(bold[-1]) # choose last run where may be most motion for illustration\n",
    "\n",
    "mean_bold = nilearn.image.mean_img(bold_images) # create mean across time\n",
    "bold_mask = nilearn.masking.compute_epi_mask(mean_bold)\n",
    "\n",
    "print(\"Before masking, our data has shape %s ...\" % (bold_images.shape,))\n",
    "bold_masked = nilearn.masking.apply_mask(bold_images, bold_mask) \n",
    "print(\"... and afterwards our data has shape %s and is a %s\" % (bold_masked.shape, type(bold_masked).__name__))\n",
    "global_signal = numpy.mean(bold_masked,axis=1)\n",
    "\n",
    "plt.figure(); plt.plot(global_signal)\n",
    "ax = plt.gca(); ax.set_title('Global signal from last run'); ax.set_ylabel('BOLD (a.u.)');  ax.set_xlabel('TR');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you did want to use any additional covariates from the fMRIPrep confounds file, for each run, then you could run something like (using global signal as example):\n",
    "\n",
    "```python\n",
    "for run in range(len(confound_files)):\n",
    "    global_signal = pandas.read_table(confound_files[run], usecols = [\"global_signal\"])\n",
    "    global_signal = (global_signal -  global_signal.mean()) /  global_signal.std()\n",
    "    confounds_per_run[run] = pandas.concat([confounds_per_run[run], global_signal], axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Instrumental (scanner) noise (and aliased biorhythms)\n",
    "\n",
    "The last main source of noise relates to instrumental noise like scanner drift (the BOLD signal typically drifts up and down over time, eg due to changes in temperature). This tends to be low-frequency, so we can remove by high-pass filtering the data. \n",
    "\n",
    "In fact, high-pass filtering also removes other types of physiological noise that is of higher frequency than our sampling rate. For example, pulse is typically 1Hz, which is above the Nyquist limit if we sample with a TR=2 (ie 0.5 Hz). This type of noise therefore becomes aliased into lower-frequencies, so also removed by high-pass filtering.\n",
    "\n",
    "Some people also apply a low-pass filter (i.e, a band-pass filter), but it is unclear whether this is important (see [Geerligs et al (2017)](https://pmc.ncbi.nlm.nih.gov/articles/PMC5518296/) for example). We will examine effects of filtering (and removal of other confounds) by using carpet plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Carpet plots\n",
    "\n",
    "A convenient way to examine the timeseries across many voxels is to produce a \"carpet plot\", showing time (TR) horizontally and voxel vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = layout.get_tr()\n",
    "\n",
    "atlas_dir = os.path.join(wd, 'atlases')\n",
    "if not os.path.exists(atlas_dir):\n",
    "    os.makedirs(atlas_dir)\n",
    "\n",
    "# Download an atlas definition of GM, WM and CSF voxels\n",
    "atlas = nilearn.datasets.fetch_icbm152_2009(data_dir = atlas_dir) # this downloads an atlas\n",
    "atlas_img = image.concat_imgs((atlas[\"gm\"], atlas[\"wm\"], atlas[\"csf\"])) # we select these segments\n",
    "map_labels={\"Gray Matter\": 1, \"White Matter\": 2, \"Cerebrospinal Fluid\": 3} # assign them labels\n",
    "atlas_data = atlas_img.get_fdata() # Get data from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These images contain probabilities of each tissue-type, which we can binarize by taking the maximum across tissue-types for each voxel\n",
    "discrete_version = numpy.argmax(atlas_data, axis = 3) + 1    # index of maximum value across GM, WM and CSF for each voxel\n",
    "discrete_version[numpy.max(atlas_data, axis = 3) == 0] = 0   # reset everything\n",
    "discrete_atlas_img = image.new_img_like(atlas_img,discrete_version) # make an image\n",
    "\n",
    "# Plot image of segments (suppressing warning about data-type)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "plotting.plot_stat_map(discrete_atlas_img,cmap='tab20b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Raw carpet\n",
    "\n",
    "We can now use these labels to indicate which parts of our carpet plot come from which tissue-type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "display = plotting.plot_carpet(\n",
    "    bold_images,\n",
    "    discrete_atlas_img,\n",
    "    t_r = TR,\n",
    "    mask_labels = map_labels,\n",
    "    axes = ax, title='Raw BOLD timeseries',\n",
    "    cmap=\"gray\")\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a particularly bad run - in some cases you might see vertical stripes indicating large motion at a certain time, or high variance in some voxels. Here there seems to be a bit more variance at start of run, and possibly some artifact around 65 TRs. \n",
    "\n",
    "#### 2.4.2 Filtered carpet\n",
    "\n",
    "Let's see the effect of applying the high-pass filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_pass_cut   = 0.008; # cut-off (typically choice - filtering too much removes dfs in our data)\n",
    "brain_masker    = NiftiMasker(standardize='zscore', high_pass = high_pass_cut, t_r = TR) # highpass filtering\n",
    "filtered_data   = brain_masker.fit_transform(bold_images)\n",
    "filtered_images = brain_masker.inverse_transform(filtered_data) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "display = plotting.plot_carpet(filtered_images, discrete_atlas_img, t_r = TR, mask_labels = map_labels, axes = ax, title='Filtered BOLD timeseries', cmap=\"gray\")\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering has not done anything obvious to the carpet plot, except perhaps introduce some filtering end-effects towards end of run (which do not matter because they same is true of all voxels we will correlate). What about the effects of removing the confounds we created above?\n",
    "\n",
    "#### 2.4.3 Deconfounded carpet\n",
    "\n",
    "Let's see what happens when we remove our 26 confounds (note we are not including global signal in our confounds here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_to_remove = numpy.asarray(confounds_per_run[-1]) # take confounds from last run to match data\n",
    "\n",
    "deconfounded_data   = brain_masker.fit_transform(bold_images, confounds = confounds_to_remove) \n",
    "deconfounded_images = brain_masker.inverse_transform(deconfounded_data) # Note it does not invert any signal processing done to the image (see https://nilearn.github.io/stable/manipulating_images/masker_objects.html)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "display = plotting.plot_carpet(deconfounded_images, discrete_atlas_img, t_r = TR, mask_labels = map_labels, axes = ax, title='Filtered, deconfounded BOLD timeseries', cmap=\"gray\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a bit cleaner (e.g. at start of run), though effects may be more dramatic in other datasets, eg from children (try your data!)\n",
    "\n",
    "This would be sufficient for resting-state data, but in our data, we still have taskBut we still have responses related to the task, ie each trial that a stimulus was presented. \n",
    "\n",
    "#### 2.4.4 Removing task-effects\n",
    "\n",
    "In our task data, if we correlated voxel time-series from the deconfounded data above, there might be high correlation between some brain regions simply because they independently responded to the same trials. To remove this, we can use the regressors we created in our GLM in previous notebooks.\n",
    "\n",
    "One way to remove this variance would be to use the LSA model we used in the MVPA notebook, i.e. adjust for a separate regressor for each trial. This would allow for fact that the amplitude of BOLD response might vary across trials of the same type (e.g, to fluctuations in attention), which the standard, one-regressor-per-condition (LSU) model would not allow for. However, the flexibility of LSA model can also fit noise (particularly with short-SOA event-related designs like ours) - at least fluctuations in the BOLD response that are not due to the trials themselves - ie remove true functional connectivity. We therefore use the standard LSU model here (but you can try the LSA model by uncommenting lines below, in which case you should observe that a strong resting-block structure emerges, suggesting it may indeed be over-fitting).\n",
    "\n",
    "Note that in general, it is difficult to remove all possible task-related signal, because in addition to amplitude variation across trials, the canonical HRF used may not be a perfect match for a particular subject and brain region. This could be resolved by using a more flexible basis set to model the HRF, though combining both LSA and a flexible basis set is infeasible except with long SOAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pandas.read_table(events_files[-1])\n",
    "\n",
    "# If want to create a LSA model with all trials of ALL conditions\n",
    "#for j, event in enumerate(events_df['trial_type']):\n",
    "#   events_df.loc[j, 'trial_type'] = event + events_df['stim_file'][j][-8:-4]\n",
    "\n",
    "events = events_df.drop(columns = ['button_pushed', 'stim_file', 'trigger', 'circle_duration', 'response_time'])\n",
    "\n",
    "slice_timing = layout.get_metadata(bold[-1])\n",
    "if slice_timing['SliceTimingCorrected']:\n",
    "  slice_time_ref = slice_timing['StartTime'] / TR\n",
    "\n",
    "# Create a design matrix without actually fitting data, which just requires from dummy frame times\n",
    "frame_times = numpy.linspace(0, (nvols - 1) * TR, nvols)\n",
    "design_matrix = nilearn.glm.first_level.make_first_level_design_matrix(frame_times, events = events, hrf_model = 'glover', drift_model = 'cosine', high_pass = 0, drift_order = None)\n",
    "# Note do not need highpass filter terms\n",
    "nilearn.plotting.plot_design_matrix(design_matrix, output_file = None)\n",
    "fig = plt.gcf(); fig.set_size_inches(8,2); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now treat these task-related regressors as confounds too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_regressors = numpy.asarray(design_matrix)\n",
    "trial_regressors = trial_regressors[:,:-1] # do not need constant\n",
    "\n",
    "print(f\"Using {trial_regressors.shape[1]} trial regressors, ie a total of {trial_regressors.shape[1] + confounds_to_remove.shape[1]} confound regressors from {nvols} TRs (though also df loss from highpass filtering)\")\n",
    "\n",
    "deconfounded_detasked_data   = brain_masker.fit_transform(bold_images, confounds = numpy.concatenate([confounds_to_remove, trial_regressors], axis=1))\n",
    "deconfounded_detasked_images = brain_masker.inverse_transform(deconfounded_detasked_data) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "display = plotting.plot_carpet(deconfounded_detasked_images, discrete_atlas_img, t_r = TR, mask_labels = map_labels, axes = ax, title='Filtered, deconfounded, task-adjusted BOLD timeseries', cmap=\"gray\")\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now estimate the correlation between all (grey-matter) voxels. We use a brain masker for this, now also adding standardisation (Z-scoring) and some spatial smoothing (to reduce noise, on assumption that functional connectivity operates over this space scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voxels_data = deconfounded_detasked_data # smoothing helps, so re-mask with smoothing\n",
    "brain_masker = NiftiMasker(standardize='zscore', high_pass = high_pass_cut, t_r = TR, smoothing_fwhm = 6) \n",
    "voxels_data  = brain_masker.fit_transform(bold_images, confounds = numpy.concatenate([confounds_to_remove, trial_regressors], axis=1))\n",
    "\n",
    "fusi_masker = NiftiMasker(fusi_ROI, high_pass = high_pass_cut, t_r = TR) # no need for smoothing since will average over ROI\n",
    "seed_data   = fusi_masker.fit_transform(bold_images, confounds = numpy.concatenate([confounds_to_remove, trial_regressors], axis=1))\n",
    "seed_data   = numpy.mean(seed_data, axis=1) # average across voxels\n",
    "seed_data   = (seed_data - numpy.mean(seed_data)) / numpy.std(seed_data) # zscore\n",
    "                                                 \n",
    "seed_to_voxel_correlations = numpy.dot(seed_data.T, voxels_data)/seed_data.shape[0]\n",
    "print(\"Correlation min = \" + str(numpy.min(seed_to_voxel_correlations)) + \", max = \" + str(numpy.max(seed_to_voxel_correlations)))\n",
    "seed_to_voxel_correlations_fisher_z = numpy.arctanh(seed_to_voxel_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now produce an image of voxels with various correlation strengths with our fusiform ROI - note of course the voxels in the right fusiform will have very high values because they are spatially part of the seed region. Although the map is a bit noisy, the left fusiform at least is highly correlated, and this cannot be due to spatial overlap with the seed. The map is noisy because we only analysed one run; more powerful statistics can be obtained by cleaning each run separately, then concatenating over runs and taking the correlation (would just take a bit longer to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy array back into a nifti image\n",
    "seed_to_voxel_correlations_fisher_z_img = brain_masker.inverse_transform(seed_to_voxel_correlations_fisher_z)\n",
    "# apply brain mask\n",
    "seed_to_voxel_correlations_fisher_z_img = nilearn.image.math_img('(in_mask * data)', in_mask=bold_mask, data=seed_to_voxel_correlations_fisher_z_img)\n",
    "seed_to_voxel_fname = 'fcon_fusi_seed.nii.gz'\n",
    "seed_to_voxel_correlations_fisher_z_img.to_filename(seed_to_voxel_fname) \n",
    "\n",
    "MNI_coord = (41.5,\t-48.5,\t-18.5)\n",
    "display = nilearn.plotting.plot_stat_map(seed_to_voxel_fname, cut_coords = MNI_coord, threshold = 0.2,\n",
    "    title=\"Z-values for functional correlation with fusiform peak voxel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Atlas-based connectome\n",
    "\n",
    "We can also divide (parcellate) the brain into a number (eg few hundred) ROIs based on an atlas in MNI space. There are a large number of such atlases, some in which the ROIs are defined anatomically, others where they are defined functionally, e.g. by clustering functional connectivity results in other datasets. Here we will use the \"Schaefer\" atlas of 500 ROIs, which is bundled with nilearn, and comes with a labelling in which those ROIs have already been defined according to 17 \"Yeo\" networks:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the 500 ROI version of Schaefer atlas, which can be organised into 17 networks according to Yeo\n",
    "dataset_schaefer = datasets.fetch_atlas_schaefer_2018(n_rois = 500, yeo_networks = 17, data_dir = atlas_out, resolution_mm=2)\n",
    "\n",
    "schaefer_atlas_filename = dataset_schaefer.maps\n",
    "temp_atlas = image.load_img(schaefer_atlas_filename)\n",
    "print(f'Shape of Atlas - {temp_atlas.shape}') # The atlas is one 3D image in which each voxel is coded by a number that indicates to which ROI it belongs\n",
    "plotting.plot_roi(schaefer_atlas_filename, colorbar = True, interpolation = 'nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now mark each ROI with a Yeo network as below (bit hacky!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky code to convert Schaefer network label bytes to integers for ordering\n",
    "schaefer_df = pandas.DataFrame(dataset_schaefer)\n",
    "#schaefer_df.head(5) # see data as bytes\n",
    "\n",
    "# new network names to define\n",
    "network_names = [\"VisCent\",\"VisPeri\",\"SomMotA\",\"SomMotB\",\"DorsAttnA\",\"DorsAttnB\",\"SalVentAttnA\",\"SalVentAttnB\",\"LimbicA\",\"LimbicB\",\"ContA\",\"ContB\",\"ContC\",\"DefaultA\",\"DefaultB\",\"DefaultC\",\"TempPar\"];\n",
    "network_numbers = list(range(1,len(network_names)+1))\n",
    "\n",
    "# number networks for each ROI\n",
    "schaefer_df['network_number'] = 0\n",
    "for idx, i_str in enumerate(network_names):\n",
    "  label = schaefer_df['labels'].astype(str).str.contains(i_str)\n",
    "  schaefer_df.loc[label, 'NetNumber'] = network_numbers[idx]\n",
    "\n",
    "# Initialise new dataframe\n",
    "new_df = schaefer_df.copy()\n",
    "new_df['ROI_Number'] = new_df.index\n",
    "new_df = new_df.sort_values(by=['NetNumber'])\n",
    "reorder_index_schaefer = new_df.ROI_Number.values\n",
    "network_numbers = new_df.NetNumber.values\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate correlation between every pair of ROIs (now using nileaarn's ConnectivityMeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how functional connectivity measured\n",
    "correlation_measure = nilearn.connectome.ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "# Get data for Schaefer ROIs\n",
    "schaefer_masker = nilearn.maskers.NiftiLabelsMasker(labels_img = schaefer_atlas_filename,\n",
    "                            standardize = 'zscore',\n",
    "                            detrend = False,\n",
    "                            high_pass = high_pass_cut,\n",
    "                            t_r = TR, verbose = 1)\n",
    "time_series_clean_schaefer = schaefer_masker.fit_transform(bold_images, confounds = numpy.concatenate([confounds_to_remove, trial_regressors], axis=1))\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix_schaefer = correlation_measure.fit_transform([time_series_clean_schaefer])[0]\n",
    "\n",
    "# Mask the main diagonal for visualization\n",
    "numpy.fill_diagonal(correlation_matrix_schaefer, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot a 500x500 connectivity matrix (connectome), with ROIs ordered as they are in original atlas, in which left ROIs are numbered first, then right ROIs. As a consequence, you can see a second \"diagonal\" stripe half-way across from the main diagonal, which represents connectivity between homologous ROIs across hemispheres. This connectivity always tends to be high, so not seeing this homologous diagonal can be a sign that something has gone wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix by ROI number (showing laterality effects)\n",
    "plt.figure(figsize=(10,8)); ax = plt.gca()\n",
    "display = plotting.plot_matrix(correlation_matrix_schaefer,\n",
    "                     vmax=0.8, vmin=-0.8, reorder=False,axes=ax, title='Ordered by left then right ROIs');\n",
    "ax.set_yticklabels(ax.get_yticklabels(),fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reorder the ROIs by networks (using the new numbered we calculated earlier). Now you should see block-like structures along the main diagonal, reflecting higher connectivity within networks than between networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix by Network\n",
    "plt.figure(figsize=(10,8)); ax = plt.gca()\n",
    "reorder_corr = correlation_matrix_schaefer[reorder_index_schaefer][:,reorder_index_schaefer];\n",
    "display = plotting.plot_matrix(reorder_corr,\n",
    "                     vmax=0.8, vmin=-0.8, reorder=False,axes=ax,title='Ordered by Network');\n",
    "ax.set_yticklabels(ax.get_yticklabels(),fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this functional connectivity notebook (for the moment) - we will analyse such connectomes in the next notebook on network (graph) theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
